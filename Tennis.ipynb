{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.6 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#!tar -xvzf baselines.tar.gz\n",
    "#!unzip multiagent.zip\n",
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "from agent import Agent\n",
    "from agent import ReplayBuffer\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters configuration for Actor and Critic Network.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "random_seed = randint(0,100)\n",
    "print(random_seed)                 \n",
    "actor_hidden_units = (512,256)\n",
    "actor_learning_rate = 1e-4\n",
    "\n",
    "critic_hidden_units = (512,256)\n",
    "critic_learning_rate = 3e-4\n",
    "\n",
    "tau = 1e-3\n",
    "weight_decay = 0\n",
    "learning_rate = 0.001\n",
    "batch_size = 1024\n",
    "buffer_size = int(1e5)\n",
    "discount = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Size\n",
      "2\n",
      "State Size\n",
      "24\n",
      "Number of Environment Agents\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "action_size = brain.vector_action_space_size\n",
    "print(\"Action Size\")\n",
    "print(action_size)\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print(\"State Size\")\n",
    "print(state_size)\n",
    "num_agents = len(env_info.agents)\n",
    "print(\"Number of Environment Agents\")\n",
    "print(num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intialization of Network for Actor\n",
      "Layers of Actor\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=2, bias=True)\n",
      "Intialization of Network for Actor\n",
      "Layers of Actor\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=2, bias=True)\n",
      "Intialization of Network for Critic\n",
      "Layers of Critic\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=514, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=1, bias=True)\n",
      "Intialization of Network for Critic\n",
      "Layers of Critic\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=514, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=1, bias=True)\n",
      "Intialization of Network for Actor\n",
      "Layers of Actor\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=2, bias=True)\n",
      "Intialization of Network for Actor\n",
      "Layers of Actor\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=2, bias=True)\n",
      "Intialization of Network for Critic\n",
      "Layers of Critic\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=514, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=1, bias=True)\n",
      "Intialization of Network for Critic\n",
      "Layers of Critic\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=514, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=1, bias=True)\n",
      "[<agent.Agent object at 0x7f692dc4f4e0>, <agent.Agent object at 0x7f692dc4fd68>]\n"
     ]
    }
   ],
   "source": [
    "tennis_multiagents = []\n",
    "\n",
    "# Create DDPG Agents for each environment agents.\n",
    "for i in range(len(env_info.agents)):\n",
    "      \n",
    "    tennis_multiagents.append(Agent(random_seed, device, action_size, state_size, actor_hidden_units, actor_learning_rate, critic_hidden_units,critic_learning_rate,weight_decay,buffer_size,batch_size,tau))\n",
    "\n",
    "print(tennis_multiagents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla K80\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Details\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize a Shared replay memory for the agents.\n",
    "# Intialize a Shared replay memory for the agents.\n",
    "shared_agents_memory = ReplayBuffer(action_size, buffer_size, batch_size, random_seed, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiagent steps\n",
    "def multiagent_step(t_step, states, actions, rewards, next_states, dones):\n",
    "    \"\"\" Add the state, next state and reward into replay buffer and sample the next experience from replay buffer.\"\"\" \n",
    "        \n",
    "    for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "        shared_agents_memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "    # Learning at each 4 steps defined in Hyperparameters.\n",
    "    t_step = (t_step + 1) % 4\n",
    "        \n",
    "    if t_step == 0:\n",
    "        # If more samples are available in replay buffer get random subset and learn\n",
    "        if len(shared_agents_memory) > batch_size:\n",
    "              \n",
    "                for agent in tennis_multiagents:\n",
    "                    \n",
    "                    experiences = shared_agents_memory.sample()\n",
    "                    \n",
    "                    agent.learn(experiences, discount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace_utils import keep_awake\n",
    "import time\n",
    "\n",
    "def train_multiagent(n_episodes=30000, max_t=2000):\n",
    "    \n",
    "    all_max_scores = []\n",
    "    all_min_scores = []\n",
    "    all_avg_scores = []\n",
    "    \n",
    "    cumulative_mean_counter = 0\n",
    "    \n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    for i_episode in keep_awake(range(1, n_episodes+1)):\n",
    "        \n",
    "        # Reset the agents at starting of each episode\n",
    "        for agent in tennis_multiagents:\n",
    "            agent.reset()\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        states = env_info.vector_observations  \n",
    "        \n",
    "        scores = np.zeros(num_agents)\n",
    "        #print(scores)\n",
    "        #print(\"Episode\")\n",
    "        timestep = time.time()\n",
    "        for i in range(max_t):\n",
    "            \n",
    "            # Next action state of multiagent\n",
    "            actions = [agent.act(np.expand_dims(states, axis=0)) for agent, states in zip(tennis_multiagents, states)]\n",
    "                \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            rewards = env_info.rewards\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            # Learning Step.\n",
    "            multiagent_step(i, states, actions, rewards, next_states, dones)\n",
    "\n",
    "            # Add up all the rewards of each agent.\n",
    "            scores += rewards\n",
    "            states = next_states\n",
    "        print(\"Agents scores after 2000 timesteps in Episode {}\".format(i_episode))    \n",
    "        print(scores)        \n",
    "        \n",
    "        all_avg_scores.append(np.mean(scores))\n",
    "        all_max_scores.append(np.max(scores))\n",
    "        all_min_scores.append(np.min(scores))\n",
    "        \n",
    "        if np.mean(all_max_scores)>=0.5:\n",
    "            cumulative_mean_counter += 1\n",
    "        \n",
    "        print('\\rEpisode: {}\\tMax: {:.2f}\\tMin: {:.2f}\\tAverage: {:.2f}\\tCumulative Average: {:.2f} in time: {:.2f}'.format(i_episode,np.max(scores),np.min(scores),np.mean(scores),np.mean(all_max_scores),timestep-time_start))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisodes(Min,Max and Avg scores till now): {}\\tMax: {:.2f}\\tMin: {:.2f}\\tAverage: {:.2f} in time: {:.2f}'.format(i_episode,np.max(all_max_scores),np.min(all_min_scores), np.mean(all_max_scores),timestep-time_start))\n",
    "        # Average over 100 episodes is greater than 0.5 over more than 100 consecutive episodes\n",
    "        if np.mean(all_max_scores)>=0.5 and len(all_max_scores) > 100 and cumulative_mean_counter > 100:\n",
    "            time_finish = time.time()\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f} in time: {:.2f}'.format(i_episode-5, np.mean(all_max_scores), time_finish-time_start))\n",
    "            torch.save(tennis_multiagents[0].actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(tennis_multiagents[0].critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break \n",
    "            \n",
    "    return all_avg_scores, all_max_scores, all_min_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents scores after 2000 timesteps in Episode 1\n",
      "[-0.51999999 -0.54999998]\n",
      "Episode: 1\tMax: -0.52\tMin: -0.55\tAverage: -0.53\tCumulative Average: -0.52 in time: 0.02\n",
      "Agents scores after 2000 timesteps in Episode 2\n",
      "[-0.67999998 -0.70999998]\n",
      "Episode: 2\tMax: -0.68\tMin: -0.71\tAverage: -0.69\tCumulative Average: -0.60 in time: 40.02\n",
      "Agents scores after 2000 timesteps in Episode 3\n",
      "[-0.62999999 -0.75999998]\n",
      "Episode: 3\tMax: -0.63\tMin: -0.76\tAverage: -0.69\tCumulative Average: -0.61 in time: 92.47\n",
      "Agents scores after 2000 timesteps in Episode 4\n",
      "[-0.73999998 -0.64999999]\n",
      "Episode: 4\tMax: -0.65\tMin: -0.74\tAverage: -0.69\tCumulative Average: -0.62 in time: 143.48\n",
      "Agents scores after 2000 timesteps in Episode 5\n",
      "[-0.63999999 -0.74999998]\n",
      "Episode: 5\tMax: -0.64\tMin: -0.75\tAverage: -0.69\tCumulative Average: -0.62 in time: 210.57\n",
      "Agents scores after 2000 timesteps in Episode 6\n",
      "[-0.66999999 -0.71999998]\n",
      "Episode: 6\tMax: -0.67\tMin: -0.72\tAverage: -0.69\tCumulative Average: -0.63 in time: 259.20\n",
      "Agents scores after 2000 timesteps in Episode 7\n",
      "[-0.62999999 -0.74999998]\n",
      "Episode: 7\tMax: -0.63\tMin: -0.75\tAverage: -0.69\tCumulative Average: -0.63 in time: 307.21\n",
      "Agents scores after 2000 timesteps in Episode 8\n",
      "[-0.70999998 -0.67999998]\n",
      "Episode: 8\tMax: -0.68\tMin: -0.71\tAverage: -0.69\tCumulative Average: -0.64 in time: 355.13\n",
      "Agents scores after 2000 timesteps in Episode 9\n",
      "[-0.76999998 -0.44999999]\n",
      "Episode: 9\tMax: -0.45\tMin: -0.77\tAverage: -0.61\tCumulative Average: -0.62 in time: 402.60\n",
      "Agents scores after 2000 timesteps in Episode 10\n",
      "[-0.61999999 -0.67999998]\n",
      "Episode: 10\tMax: -0.62\tMin: -0.68\tAverage: -0.65\tCumulative Average: -0.62 in time: 449.92\n",
      "Agents scores after 2000 timesteps in Episode 11\n",
      "[-0.71999998 -0.66999999]\n",
      "Episode: 11\tMax: -0.67\tMin: -0.72\tAverage: -0.69\tCumulative Average: -0.62 in time: 498.08\n",
      "Agents scores after 2000 timesteps in Episode 12\n",
      "[-0.59999999 -0.77999998]\n",
      "Episode: 12\tMax: -0.60\tMin: -0.78\tAverage: -0.69\tCumulative Average: -0.62 in time: 546.03\n",
      "Agents scores after 2000 timesteps in Episode 13\n",
      "[-0.78999998 -0.55999999]\n",
      "Episode: 13\tMax: -0.56\tMin: -0.79\tAverage: -0.67\tCumulative Average: -0.62 in time: 593.90\n",
      "Agents scores after 2000 timesteps in Episode 14\n",
      "[-0.51999999  1.12000004]\n",
      "Episode: 14\tMax: 1.12\tMin: -0.52\tAverage: 0.30\tCumulative Average: -0.49 in time: 641.85\n",
      "Agents scores after 2000 timesteps in Episode 15\n",
      "[-0.22999998  0.01000002]\n",
      "Episode: 15\tMax: 0.01\tMin: -0.23\tAverage: -0.11\tCumulative Average: -0.46 in time: 689.75\n",
      "Agents scores after 2000 timesteps in Episode 16\n",
      "[ 1.81000004  0.67000003]\n",
      "Episode: 16\tMax: 1.81\tMin: 0.67\tAverage: 1.24\tCumulative Average: -0.32 in time: 738.12\n",
      "Agents scores after 2000 timesteps in Episode 17\n",
      "[-0.23999999  1.53000004]\n",
      "Episode: 17\tMax: 1.53\tMin: -0.24\tAverage: 0.65\tCumulative Average: -0.21 in time: 786.44\n",
      "Agents scores after 2000 timesteps in Episode 18\n",
      "[ 2.86000005  1.77000005]\n",
      "Episode: 18\tMax: 2.86\tMin: 1.77\tAverage: 2.32\tCumulative Average: -0.04 in time: 835.17\n",
      "Agents scores after 2000 timesteps in Episode 19\n",
      "[ 2.82000005  1.30000005]\n",
      "Episode: 19\tMax: 2.82\tMin: 1.30\tAverage: 2.06\tCumulative Average: 0.11 in time: 884.43\n",
      "Agents scores after 2000 timesteps in Episode 20\n",
      "[ 1.42000003  2.08000005]\n",
      "Episode: 20\tMax: 2.08\tMin: 1.42\tAverage: 1.75\tCumulative Average: 0.21 in time: 933.69\n",
      "Agents scores after 2000 timesteps in Episode 21\n",
      "[ 1.95000004  1.73000005]\n",
      "Episode: 21\tMax: 1.95\tMin: 1.73\tAverage: 1.84\tCumulative Average: 0.29 in time: 983.42\n",
      "Agents scores after 2000 timesteps in Episode 22\n",
      "[ 2.90000005  2.27000006]\n",
      "Episode: 22\tMax: 2.90\tMin: 2.27\tAverage: 2.59\tCumulative Average: 0.41 in time: 1032.83\n",
      "Agents scores after 2000 timesteps in Episode 23\n",
      "[ 2.58000004  2.22000005]\n",
      "Episode: 23\tMax: 2.58\tMin: 2.22\tAverage: 2.40\tCumulative Average: 0.51 in time: 1082.76\n",
      "Agents scores after 2000 timesteps in Episode 24\n",
      "[ 3.51000006  0.39000003]\n",
      "Episode: 24\tMax: 3.51\tMin: 0.39\tAverage: 1.95\tCumulative Average: 0.63 in time: 1133.01\n",
      "Agents scores after 2000 timesteps in Episode 25\n",
      "[ 3.42000005  0.82000004]\n",
      "Episode: 25\tMax: 3.42\tMin: 0.82\tAverage: 2.12\tCumulative Average: 0.74 in time: 1183.15\n",
      "Agents scores after 2000 timesteps in Episode 26\n",
      "[ 3.40000005  1.91000005]\n",
      "Episode: 26\tMax: 3.40\tMin: 1.91\tAverage: 2.66\tCumulative Average: 0.85 in time: 1233.86\n",
      "Agents scores after 2000 timesteps in Episode 27\n",
      "[-0.08999998  1.54000004]\n",
      "Episode: 27\tMax: 1.54\tMin: -0.09\tAverage: 0.73\tCumulative Average: 0.87 in time: 1284.21\n",
      "Agents scores after 2000 timesteps in Episode 28\n",
      "[ 1.59000004  2.43000005]\n",
      "Episode: 28\tMax: 2.43\tMin: 1.59\tAverage: 2.01\tCumulative Average: 0.93 in time: 1334.65\n",
      "Agents scores after 2000 timesteps in Episode 29\n",
      "[ 1.05000003  2.47000005]\n",
      "Episode: 29\tMax: 2.47\tMin: 1.05\tAverage: 1.76\tCumulative Average: 0.98 in time: 1385.05\n",
      "Agents scores after 2000 timesteps in Episode 30\n",
      "[ 1.64000004  2.85000006]\n",
      "Episode: 30\tMax: 2.85\tMin: 1.64\tAverage: 2.25\tCumulative Average: 1.04 in time: 1435.68\n",
      "Agents scores after 2000 timesteps in Episode 31\n",
      "[ 2.92000005  0.97000004]\n",
      "Episode: 31\tMax: 2.92\tMin: 0.97\tAverage: 1.95\tCumulative Average: 1.10 in time: 1486.34\n",
      "Agents scores after 2000 timesteps in Episode 32\n",
      "[ 3.32000005  1.40000004]\n",
      "Episode: 32\tMax: 3.32\tMin: 1.40\tAverage: 2.36\tCumulative Average: 1.17 in time: 1537.00\n",
      "Agents scores after 2000 timesteps in Episode 33\n",
      "[ 3.59000006  1.89000005]\n",
      "Episode: 33\tMax: 3.59\tMin: 1.89\tAverage: 2.74\tCumulative Average: 1.25 in time: 1588.20\n",
      "Agents scores after 2000 timesteps in Episode 34\n",
      "[ 3.24000005  3.06000006]\n",
      "Episode: 34\tMax: 3.24\tMin: 3.06\tAverage: 3.15\tCumulative Average: 1.30 in time: 1638.77\n",
      "Agents scores after 2000 timesteps in Episode 35\n",
      "[ 3.30000006  3.43000006]\n",
      "Episode: 35\tMax: 3.43\tMin: 3.30\tAverage: 3.37\tCumulative Average: 1.37 in time: 1689.59\n",
      "Agents scores after 2000 timesteps in Episode 36\n",
      "[ 3.11000005  4.02000007]\n",
      "Episode: 36\tMax: 4.02\tMin: 3.11\tAverage: 3.57\tCumulative Average: 1.44 in time: 1740.48\n",
      "Agents scores after 2000 timesteps in Episode 37\n",
      "[ 3.28000006  4.07000007]\n",
      "Episode: 37\tMax: 4.07\tMin: 3.28\tAverage: 3.68\tCumulative Average: 1.51 in time: 1790.99\n",
      "Agents scores after 2000 timesteps in Episode 38\n",
      "[ 3.75000006  4.60000007]\n",
      "Episode: 38\tMax: 4.60\tMin: 3.75\tAverage: 4.18\tCumulative Average: 1.59 in time: 1841.71\n",
      "Agents scores after 2000 timesteps in Episode 39\n",
      "[ 4.12000006  4.28000007]\n",
      "Episode: 39\tMax: 4.28\tMin: 4.12\tAverage: 4.20\tCumulative Average: 1.66 in time: 1892.56\n",
      "Agents scores after 2000 timesteps in Episode 40\n",
      "[ 4.27000007  4.61000007]\n",
      "Episode: 40\tMax: 4.61\tMin: 4.27\tAverage: 4.44\tCumulative Average: 1.73 in time: 1943.53\n",
      "Agents scores after 2000 timesteps in Episode 41\n",
      "[ 3.60000006  4.42000007]\n",
      "Episode: 41\tMax: 4.42\tMin: 3.60\tAverage: 4.01\tCumulative Average: 1.80 in time: 1994.19\n",
      "Agents scores after 2000 timesteps in Episode 42\n",
      "[ 3.27000006  4.81000008]\n",
      "Episode: 42\tMax: 4.81\tMin: 3.27\tAverage: 4.04\tCumulative Average: 1.87 in time: 2045.18\n",
      "Agents scores after 2000 timesteps in Episode 43\n",
      "[ 3.07000005  4.57000008]\n",
      "Episode: 43\tMax: 4.57\tMin: 3.07\tAverage: 3.82\tCumulative Average: 1.93 in time: 2095.98\n",
      "Agents scores after 2000 timesteps in Episode 44\n",
      "[ 3.82000006  3.73000007]\n",
      "Episode: 44\tMax: 3.82\tMin: 3.73\tAverage: 3.78\tCumulative Average: 1.98 in time: 2146.56\n",
      "Agents scores after 2000 timesteps in Episode 45\n",
      "[ 4.01000006  3.86000007]\n",
      "Episode: 45\tMax: 4.01\tMin: 3.86\tAverage: 3.94\tCumulative Average: 2.02 in time: 2197.40\n",
      "Agents scores after 2000 timesteps in Episode 46\n",
      "[ 4.08000007  4.32000007]\n",
      "Episode: 46\tMax: 4.32\tMin: 4.08\tAverage: 4.20\tCumulative Average: 2.07 in time: 2248.03\n",
      "Agents scores after 2000 timesteps in Episode 47\n",
      "[ 4.10000006  4.78000008]\n",
      "Episode: 47\tMax: 4.78\tMin: 4.10\tAverage: 4.44\tCumulative Average: 2.13 in time: 2298.62\n",
      "Agents scores after 2000 timesteps in Episode 48\n",
      "[ 4.22000007  3.63000007]\n",
      "Episode: 48\tMax: 4.22\tMin: 3.63\tAverage: 3.93\tCumulative Average: 2.17 in time: 2349.58\n",
      "Agents scores after 2000 timesteps in Episode 49\n",
      "[ 4.55000007  3.78000007]\n",
      "Episode: 49\tMax: 4.55\tMin: 3.78\tAverage: 4.17\tCumulative Average: 2.22 in time: 2400.48\n",
      "Agents scores after 2000 timesteps in Episode 50\n",
      "[ 3.59000006  4.28000007]\n",
      "Episode: 50\tMax: 4.28\tMin: 3.59\tAverage: 3.94\tCumulative Average: 2.26 in time: 2451.18\n",
      "Agents scores after 2000 timesteps in Episode 51\n",
      "[ 3.94000007  3.92000007]\n",
      "Episode: 51\tMax: 3.94\tMin: 3.92\tAverage: 3.93\tCumulative Average: 2.30 in time: 2501.73\n",
      "Agents scores after 2000 timesteps in Episode 52\n",
      "[ 4.77000007  4.95000008]\n",
      "Episode: 52\tMax: 4.95\tMin: 4.77\tAverage: 4.86\tCumulative Average: 2.35 in time: 2552.07\n",
      "Agents scores after 2000 timesteps in Episode 53\n",
      "[ 4.53000007  4.60000007]\n",
      "Episode: 53\tMax: 4.60\tMin: 4.53\tAverage: 4.57\tCumulative Average: 2.39 in time: 2602.86\n",
      "Agents scores after 2000 timesteps in Episode 54\n",
      "[ 4.54000007  4.50000007]\n",
      "Episode: 54\tMax: 4.54\tMin: 4.50\tAverage: 4.52\tCumulative Average: 2.43 in time: 2653.78\n",
      "Agents scores after 2000 timesteps in Episode 55\n",
      "[ 4.03000006  4.04000007]\n",
      "Episode: 55\tMax: 4.04\tMin: 4.03\tAverage: 4.04\tCumulative Average: 2.46 in time: 2704.23\n",
      "Agents scores after 2000 timesteps in Episode 56\n",
      "[ 4.13000007  3.84000007]\n",
      "Episode: 56\tMax: 4.13\tMin: 3.84\tAverage: 3.99\tCumulative Average: 2.49 in time: 2754.51\n",
      "Agents scores after 2000 timesteps in Episode 57\n",
      "[ 4.24000007  3.29000006]\n",
      "Episode: 57\tMax: 4.24\tMin: 3.29\tAverage: 3.77\tCumulative Average: 2.52 in time: 2806.14\n",
      "Agents scores after 2000 timesteps in Episode 58\n",
      "[ 4.19000007  4.14000007]\n",
      "Episode: 58\tMax: 4.19\tMin: 4.14\tAverage: 4.17\tCumulative Average: 2.55 in time: 2858.23\n",
      "Agents scores after 2000 timesteps in Episode 59\n",
      "[ 4.53000007  3.81000006]\n",
      "Episode: 59\tMax: 4.53\tMin: 3.81\tAverage: 4.17\tCumulative Average: 2.58 in time: 2910.41\n",
      "Agents scores after 2000 timesteps in Episode 60\n",
      "[ 4.86000007  3.88000007]\n",
      "Episode: 60\tMax: 4.86\tMin: 3.88\tAverage: 4.37\tCumulative Average: 2.62 in time: 2962.24\n",
      "Agents scores after 2000 timesteps in Episode 61\n",
      "[ 4.98000007  4.84000007]\n",
      "Episode: 61\tMax: 4.98\tMin: 4.84\tAverage: 4.91\tCumulative Average: 2.66 in time: 3015.02\n",
      "Agents scores after 2000 timesteps in Episode 62\n",
      "[ 5.20000008  4.97000008]\n",
      "Episode: 62\tMax: 5.20\tMin: 4.97\tAverage: 5.09\tCumulative Average: 2.70 in time: 3067.02\n",
      "Agents scores after 2000 timesteps in Episode 63\n",
      "[ 5.20000008  5.29000008]\n",
      "Episode: 63\tMax: 5.29\tMin: 5.20\tAverage: 5.25\tCumulative Average: 2.74 in time: 3119.11\n",
      "Agents scores after 2000 timesteps in Episode 64\n",
      "[ 5.30000008  5.20000008]\n",
      "Episode: 64\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 2.78 in time: 3170.77\n",
      "Agents scores after 2000 timesteps in Episode 65\n",
      "[ 5.18000008  5.20000008]\n",
      "Episode: 65\tMax: 5.20\tMin: 5.18\tAverage: 5.19\tCumulative Average: 2.82 in time: 3221.91\n",
      "Agents scores after 2000 timesteps in Episode 66\n",
      "[ 5.20000008  5.30000008]\n",
      "Episode: 66\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 2.86 in time: 3274.24\n",
      "Agents scores after 2000 timesteps in Episode 67\n",
      "[ 5.30000008  5.20000008]\n",
      "Episode: 67\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 2.89 in time: 3326.54\n",
      "Agents scores after 2000 timesteps in Episode 68\n",
      "[ 5.20000008  5.30000008]\n",
      "Episode: 68\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 2.93 in time: 3379.37\n",
      "Agents scores after 2000 timesteps in Episode 69\n",
      "[ 5.30000008  5.20000008]\n",
      "Episode: 69\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 2.96 in time: 3430.82\n",
      "Agents scores after 2000 timesteps in Episode 70\n",
      "[ 5.20000008  5.30000008]\n",
      "Episode: 70\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 2.99 in time: 3482.71\n",
      "Agents scores after 2000 timesteps in Episode 71\n",
      "[ 4.99000007  5.08000008]\n",
      "Episode: 71\tMax: 5.08\tMin: 4.99\tAverage: 5.04\tCumulative Average: 3.02 in time: 3534.75\n",
      "Agents scores after 2000 timesteps in Episode 72\n",
      "[ 5.19000008  5.20000008]\n",
      "Episode: 72\tMax: 5.20\tMin: 5.19\tAverage: 5.20\tCumulative Average: 3.05 in time: 3586.56\n",
      "Agents scores after 2000 timesteps in Episode 73\n",
      "[ 5.10000008  5.17000008]\n",
      "Episode: 73\tMax: 5.17\tMin: 5.10\tAverage: 5.14\tCumulative Average: 3.08 in time: 3639.56\n",
      "Agents scores after 2000 timesteps in Episode 74\n",
      "[ 5.30000008  5.20000008]\n",
      "Episode: 74\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 3.11 in time: 3691.55\n",
      "Agents scores after 2000 timesteps in Episode 75\n",
      "[ 5.10000008  5.20000008]\n",
      "Episode: 75\tMax: 5.20\tMin: 5.10\tAverage: 5.15\tCumulative Average: 3.14 in time: 3743.76\n",
      "Agents scores after 2000 timesteps in Episode 76\n",
      "[ 5.30000008  5.20000008]\n",
      "Episode: 76\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 3.17 in time: 3795.60\n",
      "Agents scores after 2000 timesteps in Episode 77\n",
      "[ 5.10000008  5.18000008]\n",
      "Episode: 77\tMax: 5.18\tMin: 5.10\tAverage: 5.14\tCumulative Average: 3.20 in time: 3847.80\n",
      "Agents scores after 2000 timesteps in Episode 78\n",
      "[ 5.30000008  5.09000008]\n",
      "Episode: 78\tMax: 5.30\tMin: 5.09\tAverage: 5.20\tCumulative Average: 3.22 in time: 3901.43\n",
      "Agents scores after 2000 timesteps in Episode 79\n",
      "[ 5.08000008  5.07000008]\n",
      "Episode: 79\tMax: 5.08\tMin: 5.07\tAverage: 5.08\tCumulative Average: 3.25 in time: 3953.09\n",
      "Agents scores after 2000 timesteps in Episode 80\n",
      "[ 5.30000008  5.09000008]\n",
      "Episode: 80\tMax: 5.30\tMin: 5.09\tAverage: 5.20\tCumulative Average: 3.27 in time: 4005.01\n",
      "Agents scores after 2000 timesteps in Episode 81\n",
      "[ 5.20000008  4.97000008]\n",
      "Episode: 81\tMax: 5.20\tMin: 4.97\tAverage: 5.09\tCumulative Average: 3.30 in time: 4055.91\n",
      "Agents scores after 2000 timesteps in Episode 82\n",
      "[ 5.20000008  5.07000008]\n",
      "Episode: 82\tMax: 5.20\tMin: 5.07\tAverage: 5.14\tCumulative Average: 3.32 in time: 4107.41\n",
      "Agents scores after 2000 timesteps in Episode 83\n",
      "[ 5.09000008  5.08000008]\n",
      "Episode: 83\tMax: 5.09\tMin: 5.08\tAverage: 5.09\tCumulative Average: 3.34 in time: 4159.15\n",
      "Agents scores after 2000 timesteps in Episode 84\n",
      "[ 5.30000008  5.09000008]\n",
      "Episode: 84\tMax: 5.30\tMin: 5.09\tAverage: 5.20\tCumulative Average: 3.36 in time: 4210.02\n",
      "Agents scores after 2000 timesteps in Episode 85\n",
      "[ 5.10000008  5.07000008]\n",
      "Episode: 85\tMax: 5.10\tMin: 5.07\tAverage: 5.09\tCumulative Average: 3.38 in time: 4262.50\n",
      "Agents scores after 2000 timesteps in Episode 86\n",
      "[ 5.30000008  5.20000008]\n",
      "Episode: 86\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 3.41 in time: 4314.30\n",
      "Agents scores after 2000 timesteps in Episode 87\n",
      "[ 5.10000008  5.17000008]\n",
      "Episode: 87\tMax: 5.17\tMin: 5.10\tAverage: 5.14\tCumulative Average: 3.43 in time: 4366.78\n",
      "Agents scores after 2000 timesteps in Episode 88\n",
      "[ 5.19000008  5.19000008]\n",
      "Episode: 88\tMax: 5.19\tMin: 5.19\tAverage: 5.19\tCumulative Average: 3.45 in time: 4418.69\n",
      "Agents scores after 2000 timesteps in Episode 89\n",
      "[ 5.19000008  5.19000008]\n",
      "Episode: 89\tMax: 5.19\tMin: 5.19\tAverage: 5.19\tCumulative Average: 3.47 in time: 4470.27\n",
      "Agents scores after 2000 timesteps in Episode 90\n",
      "[ 5.20000008  5.18000008]\n",
      "Episode: 90\tMax: 5.20\tMin: 5.18\tAverage: 5.19\tCumulative Average: 3.49 in time: 4522.64\n",
      "Agents scores after 2000 timesteps in Episode 91\n",
      "[ 5.10000008  5.29000008]\n",
      "Episode: 91\tMax: 5.29\tMin: 5.10\tAverage: 5.20\tCumulative Average: 3.51 in time: 4574.37\n",
      "Agents scores after 2000 timesteps in Episode 92\n",
      "[ 5.20000008  4.96000008]\n",
      "Episode: 92\tMax: 5.20\tMin: 4.96\tAverage: 5.08\tCumulative Average: 3.52 in time: 4626.06\n",
      "Agents scores after 2000 timesteps in Episode 93\n",
      "[ 5.30000008  5.20000008]\n",
      "Episode: 93\tMax: 5.30\tMin: 5.20\tAverage: 5.25\tCumulative Average: 3.54 in time: 4676.33\n",
      "Agents scores after 2000 timesteps in Episode 94\n",
      "[ 5.09000008  5.09000008]\n",
      "Episode: 94\tMax: 5.09\tMin: 5.09\tAverage: 5.09\tCumulative Average: 3.56 in time: 4727.40\n",
      "Agents scores after 2000 timesteps in Episode 95\n",
      "[ 4.85000007  5.19000008]\n",
      "Episode: 95\tMax: 5.19\tMin: 4.85\tAverage: 5.02\tCumulative Average: 3.58 in time: 4779.09\n",
      "Agents scores after 2000 timesteps in Episode 96\n",
      "[ 4.76000007  5.06000008]\n",
      "Episode: 96\tMax: 5.06\tMin: 4.76\tAverage: 4.91\tCumulative Average: 3.59 in time: 4830.24\n",
      "Agents scores after 2000 timesteps in Episode 97\n",
      "[ 5.30000008  5.08000008]\n",
      "Episode: 97\tMax: 5.30\tMin: 5.08\tAverage: 5.19\tCumulative Average: 3.61 in time: 4881.89\n",
      "Agents scores after 2000 timesteps in Episode 98\n",
      "[ 5.19000008  4.96000008]\n",
      "Episode: 98\tMax: 5.19\tMin: 4.96\tAverage: 5.08\tCumulative Average: 3.63 in time: 4932.65\n",
      "Agents scores after 2000 timesteps in Episode 99\n",
      "[ 4.88000007  4.50000007]\n",
      "Episode: 99\tMax: 4.88\tMin: 4.50\tAverage: 4.69\tCumulative Average: 3.64 in time: 4983.67\n",
      "Agents scores after 2000 timesteps in Episode 100\n",
      "[ 4.88000007  4.41000007]\n",
      "Episode: 100\tMax: 4.88\tMin: 4.41\tAverage: 4.65\tCumulative Average: 3.65 in time: 5034.87\n",
      "Episodes(Min,Max and Avg scores till now): 100\tMax: 5.30\tMin: -0.79\tAverage: 3.65 in time: 5034.87\n",
      "Agents scores after 2000 timesteps in Episode 101\n",
      "[ 4.97000008  4.62000007]\n",
      "Episode: 101\tMax: 4.97\tMin: 4.62\tAverage: 4.80\tCumulative Average: 3.66 in time: 5086.30\n",
      "Agents scores after 2000 timesteps in Episode 102\n",
      "[ 5.08000008  4.74000007]\n",
      "Episode: 102\tMax: 5.08\tMin: 4.74\tAverage: 4.91\tCumulative Average: 3.68 in time: 5139.00\n",
      "Agents scores after 2000 timesteps in Episode 103\n",
      "[ 5.19000008  4.95000008]\n",
      "Episode: 103\tMax: 5.19\tMin: 4.95\tAverage: 5.07\tCumulative Average: 3.69 in time: 5190.62\n",
      "Agents scores after 2000 timesteps in Episode 104\n",
      "[ 5.00000007  4.98000007]\n",
      "Episode: 104\tMax: 5.00\tMin: 4.98\tAverage: 4.99\tCumulative Average: 3.70 in time: 5242.61\n",
      "Agents scores after 2000 timesteps in Episode 105\n",
      "[ 5.20000008  5.29000008]\n",
      "Episode: 105\tMax: 5.29\tMin: 5.20\tAverage: 5.25\tCumulative Average: 3.72 in time: 5294.42\n",
      "Agents scores after 2000 timesteps in Episode 106\n",
      "[ 5.10000008  4.97000008]\n",
      "Episode: 106\tMax: 5.10\tMin: 4.97\tAverage: 5.04\tCumulative Average: 3.73 in time: 5345.58\n",
      "Agents scores after 2000 timesteps in Episode 107\n",
      "[ 4.97000008  5.07000008]\n",
      "Episode: 107\tMax: 5.07\tMin: 4.97\tAverage: 5.02\tCumulative Average: 3.75 in time: 5398.19\n",
      "Agents scores after 2000 timesteps in Episode 108\n",
      "[ 4.87000007  5.30000008]\n",
      "Episode: 108\tMax: 5.30\tMin: 4.87\tAverage: 5.09\tCumulative Average: 3.76 in time: 5450.51\n",
      "Agents scores after 2000 timesteps in Episode 109\n",
      "[ 5.19000008  4.96000008]\n",
      "Episode: 109\tMax: 5.19\tMin: 4.96\tAverage: 5.08\tCumulative Average: 3.77 in time: 5502.96\n",
      "Agents scores after 2000 timesteps in Episode 110\n",
      "[ 5.09000008  5.18000008]\n",
      "Episode: 110\tMax: 5.18\tMin: 5.09\tAverage: 5.14\tCumulative Average: 3.79 in time: 5554.27\n",
      "Agents scores after 2000 timesteps in Episode 111\n",
      "[ 5.18000008  4.73000007]\n",
      "Episode: 111\tMax: 5.18\tMin: 4.73\tAverage: 4.96\tCumulative Average: 3.80 in time: 5605.80\n",
      "Agents scores after 2000 timesteps in Episode 112\n",
      "[ 4.98000007  4.97000008]\n",
      "Episode: 112\tMax: 4.98\tMin: 4.97\tAverage: 4.98\tCumulative Average: 3.81 in time: 5657.35\n",
      "Agents scores after 2000 timesteps in Episode 113\n",
      "[ 5.10000008  5.05000008]\n",
      "Episode: 113\tMax: 5.10\tMin: 5.05\tAverage: 5.08\tCumulative Average: 3.82 in time: 5708.72\n",
      "Agents scores after 2000 timesteps in Episode 114\n",
      "[ 5.09000008  5.06000008]\n",
      "Episode: 114\tMax: 5.09\tMin: 5.06\tAverage: 5.08\tCumulative Average: 3.83 in time: 5761.65\n",
      "Agents scores after 2000 timesteps in Episode 115\n",
      "[ 5.18000008  5.07000008]\n",
      "Episode: 115\tMax: 5.18\tMin: 5.07\tAverage: 5.13\tCumulative Average: 3.84 in time: 5813.24\n",
      "Agents scores after 2000 timesteps in Episode 116\n",
      "[ 4.88000007  5.06000008]\n",
      "Episode: 116\tMax: 5.06\tMin: 4.88\tAverage: 4.97\tCumulative Average: 3.85 in time: 5864.92\n",
      "Agents scores after 2000 timesteps in Episode 117\n",
      "[ 5.20000008  5.29000008]\n",
      "Episode: 117\tMax: 5.29\tMin: 5.20\tAverage: 5.25\tCumulative Average: 3.87 in time: 5916.42\n",
      "Agents scores after 2000 timesteps in Episode 118\n",
      "[ 4.98000007  4.97000008]\n",
      "Episode: 118\tMax: 4.98\tMin: 4.97\tAverage: 4.98\tCumulative Average: 3.88 in time: 5967.78\n",
      "Agents scores after 2000 timesteps in Episode 119\n",
      "[ 5.19000008  5.19000008]\n",
      "Episode: 119\tMax: 5.19\tMin: 5.19\tAverage: 5.19\tCumulative Average: 3.89 in time: 6020.23\n",
      "Agents scores after 2000 timesteps in Episode 120\n",
      "[ 5.18000008  5.09000008]\n",
      "Episode: 120\tMax: 5.18\tMin: 5.09\tAverage: 5.14\tCumulative Average: 3.90 in time: 6072.35\n",
      "Agents scores after 2000 timesteps in Episode 121\n",
      "[ 5.20000008  5.29000008]\n",
      "Episode: 121\tMax: 5.29\tMin: 5.20\tAverage: 5.25\tCumulative Average: 3.91 in time: 6124.70\n",
      "Agents scores after 2000 timesteps in Episode 122\n",
      "[ 4.97000008  5.19000008]\n",
      "Episode: 122\tMax: 5.19\tMin: 4.97\tAverage: 5.08\tCumulative Average: 3.92 in time: 6176.03\n",
      "Agents scores after 2000 timesteps in Episode 123\n",
      "[ 5.08000008  5.08000008]\n",
      "Episode: 123\tMax: 5.08\tMin: 5.08\tAverage: 5.08\tCumulative Average: 3.93 in time: 6227.63\n",
      "\n",
      "Environment solved in 118 episodes!\tAverage Score: 3.93 in time: 6279.60\n"
     ]
    }
   ],
   "source": [
    "avg_scores, max_scores, min_scores = train_multiagent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6905f6add8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6905babe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the average, max and min scores\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores)\n",
    "plt.plot(np.arange(1, len(min_scores)+1), min_scores)\n",
    "plt.plot(np.arange(1, len(max_scores)+1), max_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode Number')\n",
    "plt.legend(['Average', 'Min', 'Max'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('episode_score_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intialization of Network for Actor\n",
      "Layers of Actor\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=2, bias=True)\n",
      "Intialization of Network for Actor\n",
      "Layers of Actor\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=2, bias=True)\n",
      "Intialization of Network for Critic\n",
      "Layers of Critic\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=514, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=1, bias=True)\n",
      "Intialization of Network for Critic\n",
      "Layers of Critic\n",
      "Linear(in_features=24, out_features=512, bias=True)\n",
      "Linear(in_features=514, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=1, bias=True)\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 0.1450000023469329\n"
     ]
    }
   ],
   "source": [
    "agent1 = Agent(random_seed, device, action_size, state_size, actor_hidden_units, actor_learning_rate, critic_hidden_units,critic_learning_rate,weight_decay,buffer_size,batch_size,tau)\n",
    "agent1.load('checkpoint_actor.pth', 'checkpoint_critic.pth')\n",
    "\n",
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = agent1.act(states)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
